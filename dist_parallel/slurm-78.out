NODELIST=27.122.137.19,34.64.225.90
MASTER_ADDR=
Traceback (most recent call last):
  File "train.py", line 141, in <module>
    main()
  File "train.py", line 52, in main
    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
  File "/home/slurm-master/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/slurm-master/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/slurm-master/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 5 terminated with signal SIGKILL
slurmstepd: error: Detected 23 oom-kill event(s) in StepId=78.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: 27.122.137.19: task 0: Out Of Memory
srun: error: timeout waiting for task launch, started 1 of 2 tasks
srun: launch/slurm: launch_p_step_launch: StepId=78.0 aborted before step completely launched.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: Timed out waiting for job step to complete
slurmstepd: error: Detected 23 oom-kill event(s) in StepId=78.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
